Bien que Wikipedia fournit des dumps complets de son contenu, il n'a pas aisé d'utiliser l'API pour obtenir complêtement l'historique d'un article. En effet, l'API impose une limite sur la taille maximale d'une demande, ainsi que sur le nombre maximal de versions que l'on peut obtenir d'un article. Cette limite est de 1000 alors qu'un Featured Article aura bien souvent +5000 version. De ce fait, je suis obligé d'utiliser une série de commandes sucessives dans un terminal. Du à la façon dont je procède, j'ai des erreures dans la fusion des différents morceaux. Cecim'empêche d'utiliser par la suite mon script principal.  


J'utilise ces commandes afin de télécharger une version complête d'un article:

Afin de télécharger initialement un article:

curl -d "" 'http://en.wikipedia.org/w/index.php?title=Special:Export&pages=ARTICLE_NAME&offset=1&limit=1000&rvprop=ids%7Ctimestamp%7Cuser%7Csize%7Csha1%7Ccomment%7Ccontent&action=submit' > ARTICLE_NAME 1.xml

Afin de télécharger la suite:

curl -d "" 'http://en.wikipedia.org/w/index.php?title=Special:Export&pages=ARTICLE_NAME&offset=TIMESTAMP&limit=1000&rvprop=ids%7Ctimestamp%7Cuser%7Csize%7Csha1%7Ccomment%7Ccontent&action=submit' > ARTICLE_NAMEX.xml

Ensuite, j'utilise les commande BASH suivantes:

POUR Supprimer l'entête (juste pour le premier):
	sed  -i '1,39d' ARTICLE_NAME.xml
POUR Supprimer les deux dernières lignes (juste pour le dernier):
	sed -i 'N;$!P;$!D;$d' ARTICLE_NAME.xml

EXEMPLE:

sed 'N;$!P;$!D;$d' Common_cold1.xml > Common_cold_ALL.xml 

sed '1,39d' Common_cold2.xml > Co_temp.xml 

sed 'N;$!P;$!D;$d' Co_temp.xml >> Common_cold_ALL.xml 

sed '1,39d' Common_cold3.xml > Co_temp.xml 

sed 'N;$!P;$!D;$d' Co_temp.xml >> Common_cold_ALL.xml 

sed '1,39d' Common_cold4.xml > Co_temp.xml 

sed 'N;$!P;$!D;$d' Co_temp.xml >> Common_cold_ALL.xml 


sed '1,39d' Common_cold5.xml >> Common_cold_ALL.xml 

Append:
	cat ARTICLE_NAMEX.xml >> ARTICLE_NAME_ALL.xml
	
	
